{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b7fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from tqdm import tqdm as tq\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3813263",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.getcwd() + '\\\\open'\n",
    "train = os.listdir(folder)[2]\n",
    "test  = os.listdir(folder)[1]\n",
    "submit = os.listdir(folder)[0]\n",
    "\n",
    "train = pd.read_csv(folder + '/' + train)\n",
    "test = pd.read_csv(folder + '/' + test)\n",
    "submit = pd.read_csv(folder + '/' + submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9caeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train.문장.tolist()\n",
    "len(texts)\n",
    "\n",
    "# 문장 자르기\n",
    "truncated_texts = []\n",
    "for txt in texts:\n",
    "    splited = txt.split(' ')\n",
    "    if len(splited)>=40:\n",
    "        valid = ' '.join(splited[:20] + splited[-20:])\n",
    "        truncated_texts.append(valid)\n",
    "    else:\n",
    "        truncated_texts.append(txt)\n",
    "truncated_texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9dc8f",
   "metadata": {},
   "source": [
    "### embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a78c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = AutoModel.from_pretrained(\"klue/roberta-large\").embeddings.word_embeddings\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0064c",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0f4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_Q, dim_K):\n",
    "        super().__init__()\n",
    "        self.q_linear = torch.nn.Linear(dim_in, dim_Q)\n",
    "        self.k_linear = torch.nn.Linear(dim_in, dim_K)\n",
    "        self.v_linear = torch.nn.Linear(dim_in, dim_K)\n",
    "    \n",
    "    def forward(self, Q, K, V):\n",
    "        return self.scaled_dotproduct_attn(Q, K, V)\n",
    "        \n",
    "    def scaled_dotproduct_attn(self, Q, K, V):\n",
    "        numerator   = Q.bmm(K.transpose(1, 2))\n",
    "        denominator = Q.size(-1)**0.5 + 1e-08  # root d_k\n",
    "        softmax = torch.nn.functional.softmax(numerator/denominator, dim=-1)\n",
    "        return softmax.bmm(V)\n",
    "    \n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, dim_in, dim_Q, dim_K):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_Q, dim_K) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(num_heads * dim_in, dim_in)\n",
    "        \n",
    "    def forward(self, Q, K, V):\n",
    "        multi_head_result = torch.cat([ head(Q, K, V) for head in self.heads ], dim = -1)\n",
    "        return self.linear(multi_head_result)    \n",
    "    \n",
    "def position_encoding(seq_len, dim_model, device):\n",
    "    pos = torch.arange(seq_len,   dtype=torch.float, device=device).reshape(1,-1,1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1,1,-1)\n",
    "    phase = pos/(1e4 ** (dim//dim_model))\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\n",
    "\n",
    "class feed_forward(torch.nn.Module):\n",
    "    def __init__(self, dim_model=512, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dim_model, dim_feedforward),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(dim_feedforward, dim_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, sublayer, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm     = torch.nn.LayerNorm(dim)\n",
    "        self.dropout  = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, *tensors): # Query tensor first\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))\n",
    "    \n",
    "class TransformerEncoderBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model = 512,\n",
    "        num_heads = 7,\n",
    "        dim_feedforward = 2048,\n",
    "        dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim_Q = dim_K = max(dim_model//num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            sublayer = MultiHeadAttention(num_heads, dim_model, dim_Q, dim_K),\n",
    "            dim      = dim_model,\n",
    "            dropout  = dropout\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            sublayer = feed_forward(dim_model, dim_feedforward),\n",
    "            dim      = dim_model,\n",
    "            dropout  = dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, source):\n",
    "        source = self.attention(source, source, source)\n",
    "        return self.feed_forward(source)\n",
    "    \n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers = 6,\n",
    "        dim_model = 512,\n",
    "        num_heads = 7,\n",
    "        dim_feedforward = 2048,\n",
    "        dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [TransformerEncoderBlock(dim_model, num_heads, dim_feedforward, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        T, D = x.shape[1], x.shape[2]\n",
    "        x += position_encoding(T, D, self.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed1c2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers = 3,\n",
    "        dim_model = 1024,\n",
    "        num_heads = 7,\n",
    "        dim_feedforward = 2048,\n",
    "        dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        \n",
    "        # pretrained embeddings\n",
    "        self.embedding = AutoModel.from_pretrained(\"klue/roberta-large\").embeddings.word_embeddings\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.encoder  = TransformerEncoder(\n",
    "            num_layers=num_layers, \n",
    "            dim_model=dim_model, \n",
    "            num_heads=num_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "            \n",
    "    def forward(self, x_ids):\n",
    "        if x_ids.ndim ==1:\n",
    "            x_ids = x_ids.reshape(1, -1)\n",
    "        N, T = x_ids.shape\n",
    "        x = self.embedding(x_ids)\n",
    "        x = self.encoder(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30814162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TempModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d94dadd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f624392a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_ids = torch.tensor([[1,2,3,4,5]])\n",
    "out = model(x_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c50ffc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1024])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08e6b3b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6123b353",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> m = torch.nn.Conv1d(16, 33, 3, stride=2)\n",
    ">>> input = torch.randn(20, 16, 50)\n",
    ">>> output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91d6ef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 50]) torch.Size([20, 33, 24])\n"
     ]
    }
   ],
   "source": [
    "print(input.shape, output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tch",
   "language": "python",
   "name": "tch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
